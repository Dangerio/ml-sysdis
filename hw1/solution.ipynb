{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedGroupKFold, StratifiedKFold\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import textstat\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбор кейса \n",
    "\n",
    "В данной работе я ставлю перед собой следущую бизнес-цель: решить задачу **бинарной классификации вакансий на настоящие и мошеннические** для антифрод-системы сервиса с объявлениями о работе. В данной задаче классы сильно несбалансированны, так как мошеннических вакансий обычно существенно меньше, чем настоящих."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сбор данных\n",
    "\n",
    "### Датасет для исходной задачи\n",
    "\n",
    "Для своей задачи я нашел два подходящих датасет [Real / Fake Job Posting Prediction](https://www.kaggle.com/datasets/shivamb/real-or-fake-fake-jobposting-prediction) с 18 тыс. вакансий, из которых около 800 являются фейковыми\n",
    "\n",
    "Датасет имеет лицензию [CC0: Public Domain](https://creativecommons.org/publicdomain/zero/1.0/), поэтому его можно использовать для обучающих, исследовательских и коммерческих целей, не спрашивая разрешения у авторов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет и прочитаем его"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir data/real_fake_postings/ && cd data/real_fake_postings/ && kaggle datasets download shivamb/real-or-fake-fake-jobposting-prediction && unzip real-or-fake-fake-jobposting-prediction.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_fake_df = pd.read_csv(\"data/real_fake_postings/fake_job_postings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>title</th>\n",
       "      <th>location</th>\n",
       "      <th>department</th>\n",
       "      <th>salary_range</th>\n",
       "      <th>company_profile</th>\n",
       "      <th>description</th>\n",
       "      <th>requirements</th>\n",
       "      <th>benefits</th>\n",
       "      <th>telecommuting</th>\n",
       "      <th>has_company_logo</th>\n",
       "      <th>has_questions</th>\n",
       "      <th>employment_type</th>\n",
       "      <th>required_experience</th>\n",
       "      <th>required_education</th>\n",
       "      <th>industry</th>\n",
       "      <th>function</th>\n",
       "      <th>fraudulent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Marketing Intern</td>\n",
       "      <td>US, NY, New York</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We're Food52, and we've created a groundbreaki...</td>\n",
       "      <td>Food52, a fast-growing, James Beard Award-winn...</td>\n",
       "      <td>Experience with content management systems a m...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Other</td>\n",
       "      <td>Internship</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Customer Service - Cloud Video Production</td>\n",
       "      <td>NZ, , Auckland</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90 Seconds, the worlds Cloud Video Production ...</td>\n",
       "      <td>Organised - Focused - Vibrant - Awesome!Do you...</td>\n",
       "      <td>What we expect from you:Your key responsibilit...</td>\n",
       "      <td>What you will get from usThrough being part of...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Not Applicable</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Marketing and Advertising</td>\n",
       "      <td>Customer Service</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Commissioning Machinery Assistant (CMA)</td>\n",
       "      <td>US, IA, Wever</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Valor Services provides Workforce Solutions th...</td>\n",
       "      <td>Our client, located in Houston, is actively se...</td>\n",
       "      <td>Implement pre-commissioning and commissioning ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Account Executive - Washington DC</td>\n",
       "      <td>US, DC, Washington</td>\n",
       "      <td>Sales</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our passion for improving quality of life thro...</td>\n",
       "      <td>THE COMPANY: ESRI – Environmental Systems Rese...</td>\n",
       "      <td>EDUCATION: Bachelor’s or Master’s in GIS, busi...</td>\n",
       "      <td>Our culture is anything but corporate—we have ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Bachelor's Degree</td>\n",
       "      <td>Computer Software</td>\n",
       "      <td>Sales</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Bill Review Manager</td>\n",
       "      <td>US, FL, Fort Worth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SpotSource Solutions LLC is a Global Human Cap...</td>\n",
       "      <td>JOB TITLE: Itemization Review ManagerLOCATION:...</td>\n",
       "      <td>QUALIFICATIONS:RN license in the State of Texa...</td>\n",
       "      <td>Full Benefits Offered</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Bachelor's Degree</td>\n",
       "      <td>Hospital &amp; Health Care</td>\n",
       "      <td>Health Care Provider</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   job_id                                      title            location  \\\n",
       "0       1                           Marketing Intern    US, NY, New York   \n",
       "1       2  Customer Service - Cloud Video Production      NZ, , Auckland   \n",
       "2       3    Commissioning Machinery Assistant (CMA)       US, IA, Wever   \n",
       "3       4          Account Executive - Washington DC  US, DC, Washington   \n",
       "4       5                        Bill Review Manager  US, FL, Fort Worth   \n",
       "\n",
       "  department salary_range                                    company_profile  \\\n",
       "0  Marketing          NaN  We're Food52, and we've created a groundbreaki...   \n",
       "1    Success          NaN  90 Seconds, the worlds Cloud Video Production ...   \n",
       "2        NaN          NaN  Valor Services provides Workforce Solutions th...   \n",
       "3      Sales          NaN  Our passion for improving quality of life thro...   \n",
       "4        NaN          NaN  SpotSource Solutions LLC is a Global Human Cap...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Food52, a fast-growing, James Beard Award-winn...   \n",
       "1  Organised - Focused - Vibrant - Awesome!Do you...   \n",
       "2  Our client, located in Houston, is actively se...   \n",
       "3  THE COMPANY: ESRI – Environmental Systems Rese...   \n",
       "4  JOB TITLE: Itemization Review ManagerLOCATION:...   \n",
       "\n",
       "                                        requirements  \\\n",
       "0  Experience with content management systems a m...   \n",
       "1  What we expect from you:Your key responsibilit...   \n",
       "2  Implement pre-commissioning and commissioning ...   \n",
       "3  EDUCATION: Bachelor’s or Master’s in GIS, busi...   \n",
       "4  QUALIFICATIONS:RN license in the State of Texa...   \n",
       "\n",
       "                                            benefits  telecommuting  \\\n",
       "0                                                NaN              0   \n",
       "1  What you will get from usThrough being part of...              0   \n",
       "2                                                NaN              0   \n",
       "3  Our culture is anything but corporate—we have ...              0   \n",
       "4                              Full Benefits Offered              0   \n",
       "\n",
       "   has_company_logo  has_questions employment_type required_experience  \\\n",
       "0                 1              0           Other          Internship   \n",
       "1                 1              0       Full-time      Not Applicable   \n",
       "2                 1              0             NaN                 NaN   \n",
       "3                 1              0       Full-time    Mid-Senior level   \n",
       "4                 1              1       Full-time    Mid-Senior level   \n",
       "\n",
       "  required_education                   industry              function  \\\n",
       "0                NaN                        NaN             Marketing   \n",
       "1                NaN  Marketing and Advertising      Customer Service   \n",
       "2                NaN                        NaN                   NaN   \n",
       "3  Bachelor's Degree          Computer Software                 Sales   \n",
       "4  Bachelor's Degree     Hospital & Health Care  Health Care Provider   \n",
       "\n",
       "   fraudulent  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_fake_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weak Supervision\n",
    "\n",
    "#### Датасет из смежной задачи\n",
    "Из смежной задачи, которая заключается в предсказании зарплаты по данным вакансии, я нашёл датасет [LinkedIn Job Postings (2023 - 2024)](https://www.kaggle.com/datasets/arshkon/linkedin-job-postings/data) с более 100 тыс. вакансиями из LinkedIn. Датасет имеет лицензию [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/), поэтому его можно использовать для исследовательских и коммерческих целей, но выпуская наш продукт под той же лицензией с указанием ссылки на исходный датасет.\n",
    "\n",
    "В этом датасете нет информации о том, является ли заданное объявляение фейком или нет, но я воспользуюсь Weak Supervision, чтобы доразметить его. В частности, я хочу излвечь оттуда явно фейковые вакансии, что поможет увеличить датасет и уменьшить дисбаланс классов.\n",
    "\n",
    "\n",
    "#### Подход для доразметки\n",
    "Я буду помечать вакансию как фейковую, если в её описании встерчаются определенные фразы. Например, \"No experience required\", \"Unlimited earning potential\", \"Financial freedom in weeks\". После чего из таких вакансий выберу top-N с самыми короткими описаниями профилей компаний (фейковые компании обычно имеют не очень развернутые описания)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет и найтем те объявляения, которые, скорее всего, являются фейковыми"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>company_name</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>max_salary</th>\n",
       "      <th>pay_period</th>\n",
       "      <th>location</th>\n",
       "      <th>company_id</th>\n",
       "      <th>views</th>\n",
       "      <th>med_salary</th>\n",
       "      <th>...</th>\n",
       "      <th>listed_time</th>\n",
       "      <th>posting_domain</th>\n",
       "      <th>sponsored</th>\n",
       "      <th>work_type</th>\n",
       "      <th>currency</th>\n",
       "      <th>compensation_type</th>\n",
       "      <th>normalized_salary</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>fips</th>\n",
       "      <th>description_company</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>921716</td>\n",
       "      <td>Corcoran Sawyer Smith</td>\n",
       "      <td>Marketing Coordinator</td>\n",
       "      <td>Job descriptionA leading real estate firm in N...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>HOURLY</td>\n",
       "      <td>Princeton, NJ</td>\n",
       "      <td>2774458.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.713398e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>USD</td>\n",
       "      <td>BASE_SALARY</td>\n",
       "      <td>38480.0</td>\n",
       "      <td>8540.0</td>\n",
       "      <td>34021.0</td>\n",
       "      <td>With years of experience helping local buyers ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1829192</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mental Health Therapist/Counselor</td>\n",
       "      <td>At Aspen Therapy and Wellness , we are committ...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>HOURLY</td>\n",
       "      <td>Fort Collins, CO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.712858e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>USD</td>\n",
       "      <td>BASE_SALARY</td>\n",
       "      <td>83200.0</td>\n",
       "      <td>80521.0</td>\n",
       "      <td>8069.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10998357</td>\n",
       "      <td>The National Exemplar</td>\n",
       "      <td>Assitant Restaurant Manager</td>\n",
       "      <td>The National Exemplar is accepting application...</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>YEARLY</td>\n",
       "      <td>Cincinnati, OH</td>\n",
       "      <td>64896719.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.713278e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>USD</td>\n",
       "      <td>BASE_SALARY</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>45202.0</td>\n",
       "      <td>39061.0</td>\n",
       "      <td>In April of 1983, The National Exemplar began ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23221523</td>\n",
       "      <td>Abrams Fensterman, LLP</td>\n",
       "      <td>Senior Elder Law / Trusts and Estates Associat...</td>\n",
       "      <td>Senior Associate Attorney - Elder Law / Trusts...</td>\n",
       "      <td>175000.0</td>\n",
       "      <td>YEARLY</td>\n",
       "      <td>New Hyde Park, NY</td>\n",
       "      <td>766262.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.712896e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>USD</td>\n",
       "      <td>BASE_SALARY</td>\n",
       "      <td>157500.0</td>\n",
       "      <td>11040.0</td>\n",
       "      <td>36059.0</td>\n",
       "      <td>Abrams Fensterman, LLP is a full-service law f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35982263</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Service Technician</td>\n",
       "      <td>Looking for HVAC service tech with experience ...</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>YEARLY</td>\n",
       "      <td>Burlington, IA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.713452e+12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>USD</td>\n",
       "      <td>BASE_SALARY</td>\n",
       "      <td>70000.0</td>\n",
       "      <td>52601.0</td>\n",
       "      <td>19057.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     job_id            company_name  \\\n",
       "0    921716   Corcoran Sawyer Smith   \n",
       "1   1829192                     NaN   \n",
       "2  10998357  The National Exemplar    \n",
       "3  23221523  Abrams Fensterman, LLP   \n",
       "4  35982263                     NaN   \n",
       "\n",
       "                                               title  \\\n",
       "0                              Marketing Coordinator   \n",
       "1                  Mental Health Therapist/Counselor   \n",
       "2                        Assitant Restaurant Manager   \n",
       "3  Senior Elder Law / Trusts and Estates Associat...   \n",
       "4                                 Service Technician   \n",
       "\n",
       "                                         description  max_salary pay_period  \\\n",
       "0  Job descriptionA leading real estate firm in N...        20.0     HOURLY   \n",
       "1  At Aspen Therapy and Wellness , we are committ...        50.0     HOURLY   \n",
       "2  The National Exemplar is accepting application...     65000.0     YEARLY   \n",
       "3  Senior Associate Attorney - Elder Law / Trusts...    175000.0     YEARLY   \n",
       "4  Looking for HVAC service tech with experience ...     80000.0     YEARLY   \n",
       "\n",
       "            location  company_id  views  med_salary  ...   listed_time  \\\n",
       "0      Princeton, NJ   2774458.0   20.0         NaN  ...  1.713398e+12   \n",
       "1   Fort Collins, CO         NaN    1.0         NaN  ...  1.712858e+12   \n",
       "2     Cincinnati, OH  64896719.0    8.0         NaN  ...  1.713278e+12   \n",
       "3  New Hyde Park, NY    766262.0   16.0         NaN  ...  1.712896e+12   \n",
       "4     Burlington, IA         NaN    3.0         NaN  ...  1.713452e+12   \n",
       "\n",
       "  posting_domain  sponsored  work_type  currency compensation_type  \\\n",
       "0            NaN          0  FULL_TIME       USD       BASE_SALARY   \n",
       "1            NaN          0  FULL_TIME       USD       BASE_SALARY   \n",
       "2            NaN          0  FULL_TIME       USD       BASE_SALARY   \n",
       "3            NaN          0  FULL_TIME       USD       BASE_SALARY   \n",
       "4            NaN          0  FULL_TIME       USD       BASE_SALARY   \n",
       "\n",
       "  normalized_salary zip_code     fips  \\\n",
       "0           38480.0   8540.0  34021.0   \n",
       "1           83200.0  80521.0   8069.0   \n",
       "2           55000.0  45202.0  39061.0   \n",
       "3          157500.0  11040.0  36059.0   \n",
       "4           70000.0  52601.0  19057.0   \n",
       "\n",
       "                                 description_company  \n",
       "0  With years of experience helping local buyers ...  \n",
       "1                                                NaN  \n",
       "2  In April of 1983, The National Exemplar began ...  \n",
       "3  Abrams Fensterman, LLP is a full-service law f...  \n",
       "4                                                NaN  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_postings = pd.read_csv(\"data/linkedin_postings/postings.csv\")\n",
    "companies = pd.read_csv(\"data/linkedin_postings/companies/companies.csv\")[[\"company_id\", \"description\"]]\n",
    "unlabeled_postings = pd.merge(left=unlabeled_postings, right=companies, how=\"left\", on=\"company_id\", suffixes=[\"\", \"_company\"])\n",
    "unlabeled_postings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_fraudulent_job_description(text):\n",
    "    red_flag_patterns = [\n",
    "        r'\\b(earn\\s*\\$\\d+[\\d,.]*\\s*(per|/)\\s*(month|week|hour|yr|year)|make \\$[\\d,.]+\\s+(fast|quick))\\b',\n",
    "        r'\\b(pay\\s*(a|the)\\s+fee|upfront\\s+cost|security\\s+deposit|required\\s+investment)\\b',\n",
    "        r'\\b(no\\s+experience\\s+required|no\\s+qualifications\\s+needed)\\b',\n",
    "        r'\\b(guaranteed\\s+income|financial\\s+freedom\\s+in\\s+\\d+\\s+weeks)\\b',\n",
    "        r'\\b(multi[\\s-]*level\\s+marketing|mlm|pyramid\\s+scheme)\\b',\n",
    "        r'\\b(send\\s+(your\\s+)?(personal\\s+)?(information|details|bank\\s+account|ssn|social\\s+security))\\b',\n",
    "        r'\\b(work\\s+from\\s+home\\s+(with\\s+)?no\\s+(experience|interview))\\b',\n",
    "        r'\\b(cryptocurrency\\s+investment|bitcoin\\s+mining|process\\s+payments)\\b',\n",
    "        r'\\b(recruit\\s+\\d+\\s+people|build\\s+your\\s+team|referral\\s+commissions)\\b',\n",
    "        r'\\b(urgently\\s+hiring|immediate\\s+start|positions?\\s+available\\s+now)\\b',\n",
    "        r'\\b(kindly\\s+send|dear\\s+candidate,|this\\s+is\\s+not\\s+a\\s+scam)\\b',\n",
    "        r'\\b(free\\s+training\\s+materials|company\\s+will\\s+send\\s+you\\s+a\\s+check)\\b',\n",
    "        r'\\b(government\\s+approved|100%\\s+legitimate|risk-free\\s+opportunity)\\b',\n",
    "        r'\\b(wire\\s+transfers|international\\s+transactions|money\\s+transfer)\\b',\n",
    "    ]\n",
    "\n",
    "    combined_pattern = re.compile(\n",
    "        '(' + '|'.join(red_flag_patterns) + ')',\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    return bool(combined_pattern.search(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1680271/2360352750.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  suspicious_jobs.loc[:, \"description_company_len\"] = unlabeled_postings[is_fraud_job_desc].description_company.fillna(\"\").apply(lambda desc: len(desc))\n"
     ]
    }
   ],
   "source": [
    "is_fraud_job_desc = unlabeled_postings.description.fillna(\"\").apply(lambda desc: is_fraudulent_job_description(desc))\n",
    "\n",
    "suspicious_jobs = unlabeled_postings[is_fraud_job_desc]\n",
    "suspicious_jobs.loc[:, \"description_company_len\"] = unlabeled_postings[is_fraud_job_desc].description_company.fillna(\"\").apply(lambda desc: len(desc))\n",
    "\n",
    "suspicious_jobs = suspicious_jobs[suspicious_jobs.company_id.isna() | ~suspicious_jobs.company_id.duplicated()]\n",
    "topN = 300\n",
    "suspicious_jobs_idx = suspicious_jobs.description_company_len.sort_values(ascending=True)[:topN].index\n",
    "suspicious_jobs_final = suspicious_jobs.loc[suspicious_jobs_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_id</th>\n",
       "      <th>company_name</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>max_salary</th>\n",
       "      <th>pay_period</th>\n",
       "      <th>location</th>\n",
       "      <th>company_id</th>\n",
       "      <th>views</th>\n",
       "      <th>med_salary</th>\n",
       "      <th>...</th>\n",
       "      <th>posting_domain</th>\n",
       "      <th>sponsored</th>\n",
       "      <th>work_type</th>\n",
       "      <th>currency</th>\n",
       "      <th>compensation_type</th>\n",
       "      <th>normalized_salary</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>fips</th>\n",
       "      <th>description_company</th>\n",
       "      <th>description_company_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23561</th>\n",
       "      <td>3889766866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vice President Finance</td>\n",
       "      <td>Reports To: CEOFLSA: ExemptLocation: Remote Wh...</td>\n",
       "      <td>200000.0</td>\n",
       "      <td>YEARLY</td>\n",
       "      <td>Shirley, NY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>USD</td>\n",
       "      <td>BASE_SALARY</td>\n",
       "      <td>190000.0</td>\n",
       "      <td>11967.0</td>\n",
       "      <td>36103.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23618</th>\n",
       "      <td>3889770622</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Treasury Analyst</td>\n",
       "      <td>Overview of the Position:\\nFull time position ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York City Metropolitan Area</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23302</th>\n",
       "      <td>3889751726</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gift Shop Manager</td>\n",
       "      <td>Gift Shop Manager The White House Historical A...</td>\n",
       "      <td>65000.0</td>\n",
       "      <td>YEARLY</td>\n",
       "      <td>Washington, DC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>USD</td>\n",
       "      <td>BASE_SALARY</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>20001.0</td>\n",
       "      <td>11001.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36952</th>\n",
       "      <td>3895599731</td>\n",
       "      <td>A Hiring Company</td>\n",
       "      <td>Kitchen Leader</td>\n",
       "      <td>Go Chicken Go Hiring Now!\\n\\n \\n\\nAt Go Chicke...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Missouri City, TX</td>\n",
       "      <td>101478385.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>www.click2apply.net</td>\n",
       "      <td>0</td>\n",
       "      <td>FULL_TIME</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77459.0</td>\n",
       "      <td>48157.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118020</th>\n",
       "      <td>3906085362</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unemployed</td>\n",
       "      <td>Remote Positions Available!!!\\nFinancial offic...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            job_id      company_name                   title  \\\n",
       "23561   3889766866               NaN  Vice President Finance   \n",
       "23618   3889770622               NaN        Treasury Analyst   \n",
       "23302   3889751726               NaN       Gift Shop Manager   \n",
       "36952   3895599731  A Hiring Company          Kitchen Leader   \n",
       "118020  3906085362               NaN              Unemployed   \n",
       "\n",
       "                                              description  max_salary  \\\n",
       "23561   Reports To: CEOFLSA: ExemptLocation: Remote Wh...    200000.0   \n",
       "23618   Overview of the Position:\\nFull time position ...         NaN   \n",
       "23302   Gift Shop Manager The White House Historical A...     65000.0   \n",
       "36952   Go Chicken Go Hiring Now!\\n\\n \\n\\nAt Go Chicke...         NaN   \n",
       "118020  Remote Positions Available!!!\\nFinancial offic...         NaN   \n",
       "\n",
       "       pay_period                         location   company_id  views  \\\n",
       "23561      YEARLY                      Shirley, NY          NaN   33.0   \n",
       "23618         NaN  New York City Metropolitan Area          NaN   11.0   \n",
       "23302      YEARLY                   Washington, DC          NaN    6.0   \n",
       "36952         NaN                Missouri City, TX  101478385.0    2.0   \n",
       "118020        NaN                    United States          NaN   11.0   \n",
       "\n",
       "        med_salary  ...       posting_domain sponsored  work_type  currency  \\\n",
       "23561          NaN  ...                  NaN         0  FULL_TIME       USD   \n",
       "23618          NaN  ...                  NaN         0  FULL_TIME       NaN   \n",
       "23302          NaN  ...                  NaN         0  FULL_TIME       USD   \n",
       "36952          NaN  ...  www.click2apply.net         0  FULL_TIME       NaN   \n",
       "118020         NaN  ...                  NaN         0      OTHER       NaN   \n",
       "\n",
       "        compensation_type normalized_salary zip_code     fips  \\\n",
       "23561         BASE_SALARY          190000.0  11967.0  36103.0   \n",
       "23618                 NaN               NaN      NaN      NaN   \n",
       "23302         BASE_SALARY           60000.0  20001.0  11001.0   \n",
       "36952                 NaN               NaN  77459.0  48157.0   \n",
       "118020                NaN               NaN      NaN      NaN   \n",
       "\n",
       "        description_company  description_company_len  \n",
       "23561                   NaN                        0  \n",
       "23618                   NaN                        0  \n",
       "23302                   NaN                        0  \n",
       "36952                   NaN                        0  \n",
       "118020                  NaN                        0  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suspicious_jobs_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метрика качества и тестовый датасет\n",
    "#### Метрика качества\n",
    "Наша задача -- бинарная классификация с дисбалансом классов, поэтому выберем интепретируемые метрики, которые устойчивы к дисбалансу классов:\n",
    "1. F1-score\n",
    "2. Precision\n",
    "3. Recall\n",
    "\n",
    "Отметим, что мы не выбрали AUC-ROC по причине того, что в задачах, где не так важен больший класс, он может давать не совсем адекватную картину при сравнении алгоритмов.\n",
    "\n",
    "В нашей задаче важен как precision (не хотим банить настоящие объявления), так и recall (не хотим пропускать мошеннические объявления), поэтому в качестве основной метрики возьмем **f1-score**, так как она балансирует между предыдущими двумя. Precision и Recall будут второстепенными метриками.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подготовка данных перед разбиением на трейн и тест"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хочется, чтобы модель не требовала слишком много входной информации, потому что в таком случае ее будет удобнее использовать. Поэтому я оставлю только доступные пользователю и самые, по моему мнению, важные для детектирования фейка признаки: название и описание вакансии, описание компании."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = real_fake_df.fraudulent\n",
    "features = [\"title\", \"description\", \"company_profile\"]\n",
    "X = real_fake_df[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас три текстовые колонки, предобработаем их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1680271/1874289439.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[feat] = X[feat].fillna(\"\")\n",
      "/tmp/ipykernel_1680271/1874289439.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[new_feat] = X[feat].apply(preprocess_text)\n",
      "/tmp/ipykernel_1680271/1874289439.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[feat] = X[feat].fillna(\"\")\n",
      "/tmp/ipykernel_1680271/1874289439.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[new_feat] = X[feat].apply(preprocess_text)\n",
      "/tmp/ipykernel_1680271/1874289439.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[feat] = X[feat].fillna(\"\")\n",
      "/tmp/ipykernel_1680271/1874289439.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[new_feat] = X[feat].apply(preprocess_text)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Comprehensive text preprocessing\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower().strip()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "processed_features = []\n",
    "for feat in features:\n",
    "    X[feat] = X[feat].fillna(\"\")\n",
    "    new_feat = feat + \"_processed\"\n",
    "    processed_features.append(new_feat)\n",
    "    X[new_feat] = X[feat].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала проверим нет ли в датасете дубликатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2308"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.duplicated(subset=processed_features, keep='first').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Они есть, поэтому уберем их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_dupl_mask = ~X.duplicated(subset=processed_features, keep=\"first\")\n",
    "X = X[not_dupl_mask].reset_index(drop=True)\n",
    "y = y[not_dupl_mask].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также посмотрим на очень похожие объекты и по возможности от них избавимся"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X.copy()\n",
    "y_new = y.copy()\n",
    "\n",
    "\n",
    "X_new['combined'] = (\n",
    "    X_new['title_processed'] + ' ' +\n",
    "    X_new['description_processed'] + ' ' +\n",
    "    X_new['company_profile_processed']\n",
    ")\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english', min_df=0.01, max_df=0.85)\n",
    "tfidf_matrix = tfidf.fit_transform(X_new['combined'])\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "threshold = 0.95\n",
    "duplicates = set()\n",
    "\n",
    "for i in range(cosine_sim.shape[0]):\n",
    "    if i not in duplicates:\n",
    "        similar = np.where(cosine_sim[i] > threshold)[0]\n",
    "        similar = [s for s in similar if s != i and s not in duplicates]\n",
    "        duplicates.update(similar)\n",
    "\n",
    "X_new_clean = X_new.drop(index=list(duplicates), columns=\"combined\").reset_index(drop=True)\n",
    "y_new_clean = y_new.drop(index=list(duplicates), columns=\"combined\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разбиение датасета на обучающий и тестовый\n",
    "Нам не подходит случайное разбиение на трейн и тест, так как, скорее всего, вакансии, принадлежащие отной компании, либо все фейковые, либо все настоящие. Поэтому нельзя допустить, чтобы вакансии одной компании были одновременно и в трейне и в тесте (иначе будет утечка).\n",
    "\n",
    "Именно поэтому мы будем разбивать так, что строки с одинаковым описанием компании обязательно пойдут в ровно одну из выборок. Также по возможности будем поддерживать долю положительного класса приблизтельно одинаковой в выборках. Для этого воспользуемся `StratifiedGroupKFold`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train target mean: 0.0498\n",
      "Test target mean: 0.0386\n",
      "Original target mean: 0.0475\n",
      "Test share: 0.20795932957343713\n"
     ]
    }
   ],
   "source": [
    "company_profile_to_group = {desc: i for i, desc in enumerate(X_new_clean.company_profile.unique().tolist(), 0)}\n",
    "groups = X_new_clean.company_profile.apply(lambda prof: company_profile_to_group[prof])\n",
    "n_splits = int(1 / 0.3)\n",
    "\n",
    "splitter = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "train_idx, test_idx = next(splitter.split(X_new_clean, y_new_clean, groups))\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test = X_new_clean.iloc[train_idx], X_new_clean.iloc[test_idx]\n",
    "y_train, y_test = y_new_clean.iloc[train_idx], y_new_clean.iloc[test_idx]\n",
    "groups_train = groups.iloc[train_idx]\n",
    "\n",
    "\n",
    "print(f\"Train target mean: {y_train.mean():.4f}\")\n",
    "print(f\"Test target mean: {y_test.mean():.4f}\")\n",
    "print(f\"Original target mean: {y_new_clean.mean():.4f}\")\n",
    "\n",
    "print(f\"Test share: {X_test.shape[0] / X_new_clean.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что доли положительного класса приблизительно одинаковы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка к обучению"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Добавление Weak Supervision датасета в обучающую выборку\n",
    "Ранее с помощью Weak Supervision из другого датасета мы получили объявления о работе, которые мы будем считать фейковыми. Добавим их в нашу обучающую выборку, преобразуя по возможности столбцы одного датасета в соотвествующие столбцы другого."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_train_part = suspicious_jobs_final.copy()\n",
    "old_feature_to_new = {\n",
    "    \"title\": \"title\",\n",
    "    \"description\": \"description\",\n",
    "    \"company_profile\": \"description_company\"\n",
    "}\n",
    "\n",
    "for feat in features:\n",
    "    new_X_train_part[feat] = new_X_train_part[old_feature_to_new[feat]].fillna(\"\")\n",
    "    new_feat = feat + \"_processed\"\n",
    "    new_X_train_part[new_feat] = new_X_train_part[feat].apply(preprocess_text)\n",
    "\n",
    "new_X_train_part = new_X_train_part[features + processed_features]\n",
    "new_y_train_part = pd.Series(np.ones((new_X_train_part.shape[0])))\n",
    "\n",
    "X_train = pd.concat((X_train, new_X_train_part)).reset_index(drop=True)\n",
    "groups_train = pd.concat([groups_train, pd.Series(np.arange(groups_train.max() + 1, len(new_X_train_part) + groups_train.max() + 1))]).reset_index(drop=True)\n",
    "y_train = pd.Series(pd.concat((y_train, new_y_train_part)).reset_index(drop=True), name='fraudulent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедимся, что в трейне дубликатов не добавилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним данные в файлики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"data/my/before_feature_engineering/train.csv\", sep=\",\", header=True)\n",
    "X_test.to_csv(\"data/my/before_feature_engineering/test.csv\", sep=\",\", header=True)\n",
    "\n",
    "y_train.to_csv(\"data/my/before_feature_engineering/train_labels.csv\", sep=\",\", header=True)\n",
    "y_test.to_csv(\"data/my/before_feature_engineering/test_labels.csv\", sep=\",\", header=True)\n",
    "\n",
    "groups_train.to_csv(\"data/my/before_feature_engineering/train_groups.csv\", sep=\",\", header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Преобразование данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все необходимые преобразования исходных данных мы провели до разбиения на трейн и тест. Теперь займемся придумыванием новых признаков. Воспользуемся следующими методами:\n",
    "- с помощью tf-idf построим векторное представление текстовых признаков, далее уменьшим их размерность с помощью PCA \n",
    "- посчитаем некоторые текстовые статистики, которые могут быть полезны для детекстирования фрода. Например, длина текста может быть важная -- пустые описания часто характерны для фейковых вакансий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextStatsTransformer(BaseEstimator, TransformerMixin):\n",
    "    def _download_nltk_resources(self):\n",
    "        try:\n",
    "            nltk.data.find('sentiment/vader_lexicon.zip')\n",
    "        except LookupError:\n",
    "            print(\"Downloading NLTK VADER lexicon...\")\n",
    "            nltk.download('vader_lexicon', quiet=False)\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self._download_nltk_resources()\n",
    "        self.sia = SentimentIntensityAnalyzer()\n",
    "        \n",
    "    \n",
    "    def _process_column(self, X):\n",
    "        stats = pd.DataFrame(index=X.index)\n",
    "        text = X.fillna('').apply(preprocess_text)\n",
    "        \n",
    "        stats['char_count'] = text.apply(len)\n",
    "        stats['word_count'] = text.apply(lambda x: len(x.split()))\n",
    "        stats['unique_words'] = text.apply(lambda x: len(set(x.split())))\n",
    "        stats['readability'] = text.apply(textstat.flesch_reading_ease)\n",
    "        stats['sentiment'] = text.apply(lambda x: self.sia.polarity_scores(x)['compound'])\n",
    "        stats['has_url'] = text.str.contains(r'http[s]?://').astype(int)\n",
    "        \n",
    "        stats['exclamation_count'] = text.str.count(r'!')\n",
    "        stats['all_caps_ratio'] = text.apply(\n",
    "            lambda x: sum(1 for w in x.split() if w.isupper()) / len(x.split()) \n",
    "            if len(x.split()) > 0 else 0\n",
    "        )\n",
    "        \n",
    "        return stats\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self._process_column(X)\n",
    "\n",
    "\n",
    "def create_tfidf_pipeline(max_features=1000, n_components=50):\n",
    "    return Pipeline([\n",
    "        (\n",
    "            'tfidf',\n",
    "            TfidfVectorizer(\n",
    "                preprocessor=preprocess_text,\n",
    "                max_features=max_features,\n",
    "                stop_words='english',\n",
    "                ngram_range=(1, 2),\n",
    "                analyzer='word'\n",
    "            )\n",
    "        ),\n",
    "        (\n",
    "            'to_dense', \n",
    "            FunctionTransformer(\n",
    "                lambda x: x.toarray(), \n",
    "                accept_sparse=True\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            'pca', \n",
    "            PCA(\n",
    "                n_components=n_components,\n",
    "                whiten=True,\n",
    "                random_state=42\n",
    "            )\n",
    "        )\n",
    "    ])\n",
    "\n",
    "\n",
    "tfidf_features = ColumnTransformer([\n",
    "    ('title_tfidf', create_tfidf_pipeline(500, 5), 'title'),\n",
    "    ('desc_tfidf', create_tfidf_pipeline(2000, 75), 'description'),\n",
    "    ('profile_tfidf', create_tfidf_pipeline(2000, 75), 'company_profile')\n",
    "])\n",
    "\n",
    "stats_features = ColumnTransformer([\n",
    "    (f'{col}_stats', TextStatsTransformer(), col)\n",
    "    for col in features\n",
    "])\n",
    "\n",
    "final_pipeline = Pipeline([\n",
    "    (\n",
    "        'features', \n",
    "        FeatureUnion([\n",
    "            ('tfidf', tfidf_features),\n",
    "            ('stats', stats_features)\n",
    "        ])\n",
    "    ),\n",
    "    (\n",
    "        'scaler', \n",
    "        StandardScaler()\n",
    "    )\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tr = pd.DataFrame(final_pipeline.fit_transform(X_train))\n",
    "X_test_tr = pd.DataFrame(final_pipeline.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tr.to_csv(\"data/my/after_feature_engineering/train.csv\", sep=\",\", header=True)\n",
    "X_test_tr.to_csv(\"data/my/after_feature_engineering/test.csv\", sep=\",\", header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Проверка на утечку данных\n",
    "\n",
    "Кажется, утечки данных не происходит, так как:\n",
    "1. Мы предсказываем фейковость объявления по его названию, описаниям компании и работы. В этих признаках нет явной утечки таргета с точки зрения здравого смысла\n",
    "2. Дедупликация данных была произведена до разбиения на трейн и тест (похожие объекты могут вызвать утечку)\n",
    "3. Feature engineering делается только по обучающей выборке, а к тестовой только применяется (fit_transform/transform)\n",
    "4. Мы не использовали информацию о таргете в предобработке данных/фича инжиниринге\n",
    "5. С потенциальной утечкой в дубликаций описаний компании мы поборолись при разбиении на трейн и тест"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Бейзлайн\n",
    "\n",
    "В качестве бейзлайна я возьму логистическую регрессию, так как:\n",
    "- Она интерпретируема, так как коэффициенты отражают важность признаков (т.к. мы их пошкалировали)\n",
    "- Она быстро учится, в сравнение с многими другими алгоритмами\n",
    "- Хорошо работает с большим количеством признаков (актуально, так как у нас много tf-idf признаков)\n",
    "- На выход выдает вероятность, что позволяет ее удобно калибровать (особенно важно для нашей задачи с явным precision/recall трейдоффом)\n",
    "\n",
    "Другие варианты бейзлайна и причины, почему их я не выбрал:\n",
    "1. Наивный баес\n",
    "    - Плохо работает с смешанными типами фичей (в нашем случае tf-idf и статистики)\n",
    "    - Плохо работает с зависимыми признаками\n",
    "2. Решающее дерево\n",
    "    - Переобучается, в особенности под редкие n-граммы\n",
    "3. Случайный лес\n",
    "    - Медленее учится\n",
    "    - Менее интепретируем\n",
    "    - Лучше его попробовать после линейной регрессии\n",
    "4. Нейронные сети / градиентный бустинг\n",
    "    - оверкилл для бейзлайна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение первой модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим логистическую регрессию, а аткже подберем коэффициент регуляризации и вес класса по кросс-валидации. Будем логировать метрикики в wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdangerio\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.1s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aksenovan/ml-sysdis/hw1/wandb/run-20250316_164341-141res00</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dangerio/job-fake-prediction/runs/141res00' target=\"_blank\">brisk-energy-29</a></strong> to <a href='https://wandb.ai/dangerio/job-fake-prediction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dangerio/job-fake-prediction' target=\"_blank\">https://wandb.ai/dangerio/job-fake-prediction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dangerio/job-fake-prediction/runs/141res00' target=\"_blank\">https://wandb.ai/dangerio/job-fake-prediction/runs/141res00</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aksenovan/my_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>C</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂████</td></tr><tr><td>best_C</td><td>▁</td></tr><tr><td>best_mean_f1</td><td>▁▇██████████████████████████████████████</td></tr><tr><td>fold</td><td>▃█▃▆█▁▁▅▆██▁▆█▃▅█▁▁▁▆▃▃▁▆▅▆▁▆█▁▅█▆█▆▆▃██</td></tr><tr><td>fold_f1</td><td>▆▅▆▃▂▁█▅▆▄▄▅▄▅▄▃█▄▄▆▃▅█▄▄▄▅▄▄▃▅▄▃▄▅▅▅▅▆▆</td></tr><tr><td>fold_precision</td><td>▂▂▃▄▁▂▁▂▃▄▂▃█▂▂▂▃▃█▆▃▃▂▂▂▄▁▁▂▂▂▂▇▁▂▆▂▃▂▂</td></tr><tr><td>fold_recall</td><td>█▇█▃▂█▇▇█▇▇▇▁▆▇▁▂▇▆▆▇▂▆▅█▇▇█▂▂▇▆▆▇▆▅▇▇▇▇</td></tr><tr><td>mean_f1</td><td>▁▇█▆▅▆▆██▇▇▆███▆▇▇██▆▇▇██▇▆▇▇██▇▆▇▇██▇▆▇</td></tr><tr><td>mean_precision</td><td>▅▂▃▁▁▁█▃▃▂▂▂█▃▃▂▂▂▇▃▂▂▂▂▇▃▂▂▂▂▃▃▂▂▂▃▃▂▂▂</td></tr><tr><td>mean_recall</td><td>▁▇▆██▂▇▇▇█▇▃▇▆▇▇▇▃▇▆▇▇▇▃▇▇▇▇▇▃▆▇▇▇▇▇▆▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>C</td><td>1000</td></tr><tr><td>best_C</td><td>0.01</td></tr><tr><td>best_class_weight</td><td>{0: 1, 1: 10}</td></tr><tr><td>best_mean_f1</td><td>0.44367</td></tr><tr><td>class_weight</td><td>{0: 1, 1: 35}</td></tr><tr><td>fold</td><td>5</td></tr><tr><td>fold_f1</td><td>0.33846</td></tr><tr><td>fold_precision</td><td>0.21802</td></tr><tr><td>fold_recall</td><td>0.75625</td></tr><tr><td>mean_f1</td><td>0.37322</td></tr><tr><td>mean_precision</td><td>0.23856</td></tr><tr><td>mean_recall</td><td>0.85956</td></tr><tr><td>status</td><td>completed</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">brisk-energy-29</strong> at: <a href='https://wandb.ai/dangerio/job-fake-prediction/runs/141res00' target=\"_blank\">https://wandb.ai/dangerio/job-fake-prediction/runs/141res00</a><br> View project at: <a href='https://wandb.ai/dangerio/job-fake-prediction' target=\"_blank\">https://wandb.ai/dangerio/job-fake-prediction</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250316_164341-141res00/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LogisticRegression(\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "with wandb.init(project=\"job-fake-prediction\", \n",
    "          config={\n",
    "              \"model_type\": \"logistic_regression\",\n",
    "              \"validation\": \"stratified_kfold\",\n",
    "              \"k_folds\": 5\n",
    "          }):\n",
    "    classes = np.unique(y_train)\n",
    "    balanced_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "    class_ratio = balanced_weights[1]/balanced_weights[0]\n",
    "\n",
    "    class_weight_grid = [\n",
    "        None,\n",
    "        'balanced',\n",
    "        {0: 1, 1: 10},\n",
    "        {0: 1, 1: 30},\n",
    "        {0: 1, 1: 50},\n",
    "        {0: 1, 1: int(class_ratio*1.5)},\n",
    "        {0: 1, 1: int(class_ratio*3)}\n",
    "    ]\n",
    "\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "        'class_weight': class_weight_grid\n",
    "    }\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    best_score = 0\n",
    "    best_params = {}\n",
    "\n",
    "    for c in param_grid['C']:\n",
    "        for class_weight in param_grid['class_weight']:\n",
    "            fold_metrics = {'f1': [], 'precision': [], 'recall': []}\n",
    "            \n",
    "            wandb.log({\n",
    "                'C': c,\n",
    "                'class_weight': str(class_weight),\n",
    "                'status': 'started'\n",
    "            }, commit=False)\n",
    "            \n",
    "            for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_tr, y_train)):\n",
    "                X_train_f, X_val_f = X_train_tr.iloc[train_idx], X_train_tr.iloc[val_idx]\n",
    "                y_train_f, y_val_f = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "                \n",
    "                model_f = model.set_params(\n",
    "                    C=c,\n",
    "                    class_weight=class_weight\n",
    "                )\n",
    "                \n",
    "                model_f.fit(X_train_f, y_train_f)\n",
    "                y_pred = model_f.predict(X_val_f)\n",
    "                \n",
    "                fold_metrics['f1'].append(f1_score(y_val_f, y_pred))\n",
    "                fold_metrics['precision'].append(precision_score(y_val_f, y_pred))\n",
    "                fold_metrics['recall'].append(recall_score(y_val_f, y_pred))\n",
    "                \n",
    "                wandb.log({\n",
    "                    'C': c,\n",
    "                    'class_weight': str(class_weight),\n",
    "                    'fold': fold + 1,\n",
    "                    'fold_f1': fold_metrics['f1'][-1],\n",
    "                    'fold_precision': fold_metrics['precision'][-1],\n",
    "                    'fold_recall': fold_metrics['recall'][-1]\n",
    "                })\n",
    "            \n",
    "            mean_metrics = {\n",
    "                'mean_f1': np.mean(fold_metrics['f1']),\n",
    "                'mean_precision': np.mean(fold_metrics['precision']),\n",
    "                'mean_recall': np.mean(fold_metrics['recall'])\n",
    "            }\n",
    "            \n",
    "            wandb.log({\n",
    "                **mean_metrics,\n",
    "                'C': c,\n",
    "                'class_weight': str(class_weight),\n",
    "                'status': 'completed'\n",
    "            })\n",
    "            \n",
    "            if mean_metrics['mean_f1'] > best_score:\n",
    "                best_score = mean_metrics['mean_f1']\n",
    "                best_params = {'C': c, 'class_weight': class_weight}\n",
    "            wandb.log({\"best_mean_f1\": best_score})\n",
    "                \n",
    "    wandb.log({\n",
    "        'best_C': best_params['C'],\n",
    "        'best_class_weight': str(best_params['class_weight']),\n",
    "        'best_mean_f1': best_score\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скрины метрик (step здесь имеет смысл очередного сплита кросс-валидации):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/baseline/f1.png)\n",
    "![](images/baseline/precision.png)\n",
    "![](images/baseline/recall.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучим с наилучшими параметрами логистическую регрессию на всем трейне и посчитаем метрики на трейне и тесте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1-Score: 0.500\n",
      "Test F1-Score: 0.237\n",
      "\n",
      "Train Precision: 0.350\n",
      "Test Precision: 0.161\n",
      "\n",
      "Train Recall: 0.876\n",
      "Test Recall: 0.446\n"
     ]
    }
   ],
   "source": [
    "best_model = model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    **best_params\n",
    ")\n",
    "\n",
    "best_model.fit(X_train_tr, y_train)\n",
    "\n",
    "y_train_pred = best_model.predict(X_train_tr)\n",
    "y_test_pred = best_model.predict(X_test_tr)\n",
    "\n",
    "print(f\"Train F1-Score: {f1_score(y_train, y_train_pred):.3f}\")\n",
    "print(f\"Test F1-Score: {f1_score(y_test, y_test_pred):.3f}\", end=\"\\n\\n\")\n",
    "\n",
    "print(f\"Train Precision: {precision_score(y_train, y_train_pred):.3f}\")\n",
    "print(f\"Test Precision: {precision_score(y_test, y_test_pred):.3f}\", end=\"\\n\\n\")\n",
    "\n",
    "print(f\"Train Recall: {recall_score(y_train, y_train_pred):.3f}\")\n",
    "print(f\"Test Recall: {recall_score(y_test, y_test_pred):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На глаз результаты кажутся неплохими -- мы выявлили почти половину фродовых вакансий, правда ошибаясь в 74% случаев. Однако учитывая, что фрода в датасете довольно мало, в абсолютном выражении мы ошибаемся на так часто."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
